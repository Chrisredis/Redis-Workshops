{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "view-in-github"
   },
   "source": [
    "<a href=\"https://colab.research.google.com/github/Redislabs-Solution-Architects/Redis-Workshops/blob/main/05-LangChain_Redis/05.02_Dolly_v2_LangChain_Redis.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "id": "R2-i8jBl9GRH"
   },
   "source": [
    "# Document Question Answering with LangChain, Dolly LLM and Redis\n",
    "\n",
    "![Redis](https://redis.io/wp-content/uploads/2024/04/Logotype.svg?auto=webp&quality=85,75&width=120)\n",
    "\n",
    "This notebook would use `databricks/dolly-v2-3b` LLM from HuggingFace, Redis with Vector Similarity Search and LangChain to answer questions about the information contained in a document."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "id": "RZsd9twK9-sJ"
   },
   "source": [
    "### Install Dependencies\n",
    "\n",
    "WARNING!!! We have to downgrade the version of one of the libraries here and that requires restart of the notebook runtime. At the end of the next cell you'll be prompted to restart the runtime. Restart and continue with the execution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "C8uONNrVbe08",
    "outputId": "e59a08f9-d022-44aa-8a2b-f8990f5409c7"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m240.3/240.3 kB\u001b[0m \u001b[31m22.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.0/1.0 MB\u001b[0m \u001b[31m50.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.7/1.7 MB\u001b[0m \u001b[31m81.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m86.0/86.0 kB\u001b[0m \u001b[31m12.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.2/7.2 MB\u001b[0m \u001b[31m83.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m227.6/227.6 kB\u001b[0m \u001b[31m26.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.0/1.0 MB\u001b[0m \u001b[31m27.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m90.0/90.0 kB\u001b[0m \u001b[31m11.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.3/1.3 MB\u001b[0m \u001b[31m65.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m236.8/236.8 kB\u001b[0m \u001b[31m26.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.8/7.8 MB\u001b[0m \u001b[31m113.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.3/1.3 MB\u001b[0m \u001b[31m91.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m114.5/114.5 kB\u001b[0m \u001b[31m16.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m268.8/268.8 kB\u001b[0m \u001b[31m35.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m149.6/149.6 kB\u001b[0m \u001b[31m23.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m49.1/49.1 kB\u001b[0m \u001b[31m6.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25h  Building wheel for sentence_transformers (setup.py) ... \u001b[?25l\u001b[?25hdone\n"
     ]
    }
   ],
   "source": [
    "!pip install -q redis \"langchain==0.0.227\" tiktoken sentence_transformers transformers accelerate\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 328,
     "referenced_widgets": [
      "0f8e138212aa450bad00cb24c78bf1bb",
      "50fd9e53dda746d29f4069477b0fc94f",
      "839d0d5184014681ab165328542cdc48",
      "d79e62e0c19a4aab95bbda6dbaab4949",
      "027180e40c1a43a492a881b81e2fed4f",
      "2454fa64f3044014ba4cf4594ca3e624",
      "9fdaa8798c674709913ed2594cfe3322",
      "8200943986a1482dad956d3d8a19c1f2",
      "5afb6858512a46d09c3f3aa49965e3e2",
      "2fc793c1e1004384ad47cb79922bb78d",
      "c921aaeb7b144e17bc944c611ad32d1f",
      "a66a52476b594dd48de38e1f05484645",
      "274baa9395764a52995d0d9a31847e40",
      "f9ea74173d5a47aebe17ca22eb19da33",
      "ed3e23ec9c6a403ca358c53ba4b29fda",
      "31684d7c8b90464ea564728fe0c6683b",
      "0e1f31e4603c4e5d90aeaae07f07e06d",
      "736a6b8d6b2c4f3cbe4e047247be91b1",
      "522c60d8c6bf4b759e8661724edde381",
      "c16e8ca82e734effb33cfd243636211f",
      "60d09122f6b245a3ae59d2714cfe0beb",
      "2d02cdadee42463e87eef6e7e48fb4f2",
      "07c2cb5c7a274f5d98b028ee5f83397e",
      "dab71e1e0f314672ad2aec2776fe37c4",
      "9f3e2e3c72744a518a08c40c0c528027",
      "9ebb1baa960f44f781c050cf088dbdd8",
      "bfa8db1ac3024849a3f3d87badd7ccea",
      "affdb237e4c04ce095925c69ff5f2458",
      "ce43240076624306b04573d76b75e6b0",
      "15c81d22269d4796ba00238143171313",
      "28e54c8d6e9c406cba94899a822f9a7a",
      "e8bdbc5eea0944a6a3520a2e6f586543",
      "e3c883a9bd714154a196d56b3bda7cdd",
      "f0f65395771d470eb2f6b8bb45e841cf",
      "8487320e1e5d43ec83229c3dbd5371a8",
      "553df3a807a541a3a4297e10562e3687",
      "0858f90f29bc4ae594255bf01533945d",
      "fe678785050549c38872036dfaae0fd4",
      "3f7020eaf1a64ba0bb32c0d91ffc74bd",
      "b1f6f844b5294be7962d277c9c9a8e83",
      "be434037448742eb8e9fac9bb6854227",
      "236424c2c486464dab615d085b4958f1",
      "5136b2c9b47e42e49ac6e2a2710823ce",
      "c3a538e51e9840bfb8cc3c684269cbf6",
      "a08e7ed13fb44cab9a95e9baaab29324",
      "91331384cafd4b61a3381e9a3cd4402e",
      "420496baa70546b282142ab52440ae07",
      "015ea5425f2f4386a2ba7defcb37b227",
      "0f6931b3a434436f8fe319530ea205bd",
      "3f1b5eb64a2a42db94b83ebb577e794f",
      "1e622c7dd26b44fc8b64b2a62d6f4963",
      "8c6ce3c9e8d34c699a1c8eea64e86775",
      "c463316b09f0429b8116f5e898528ef5",
      "d6f49c2766484cec8330f97442a8e28a",
      "5070217065864711a40da133b1e59b5b",
      "5368d4a2d40c4fddaa6a663738de735c",
      "d4f92482819540f09cc4c87cf47d658a",
      "adf61725b7104baeb8947c46a4fd8151",
      "b57c0c03e3e543449b8b2b28a484f351",
      "d8a21623811d469bb754a4da765691c3",
      "5bc229451d17432fb50538cfa37105b3",
      "c83e68491aba4e3abeb262a640ed324b",
      "2e61d3cb28c44dfc9474fb2fac7f5fd5",
      "1631278178bf4140a94fe1a720503048",
      "4839e330407e40e39327ad6ab0e2c25f",
      "bf6f11b2fa284f52a4d7a797fce15cc1"
     ]
    },
    "id": "uLl2FQoHz-xW",
    "outputId": "d783bdaf-0d86-4812-faa2-5545970d51d5"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0f8e138212aa450bad00cb24c78bf1bb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)lve/main/config.json:   0%|          | 0.00/819 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a66a52476b594dd48de38e1f05484645",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)instruct_pipeline.py:   0%|          | 0.00/9.16k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "A new version of the following files was downloaded from https://huggingface.co/databricks/dolly-v2-3b:\n",
      "- instruct_pipeline.py\n",
      ". Make sure to double-check they do not contain any added malicious code. To avoid downloading new versions of the code file, you can pin a revision.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "07c2cb5c7a274f5d98b028ee5f83397e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading pytorch_model.bin:   0%|          | 0.00/5.68G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:accelerate.utils.modeling:The model weights are not tied. Please use the `tie_weights` method before using the `infer_auto_device` function.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f0f65395771d470eb2f6b8bb45e841cf",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)okenizer_config.json:   0%|          | 0.00/450 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a08e7ed13fb44cab9a95e9baaab29324",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)/main/tokenizer.json:   0%|          | 0.00/2.11M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5368d4a2d40c4fddaa6a663738de735c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)cial_tokens_map.json:   0%|          | 0.00/228 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from langchain.llms import HuggingFacePipeline\n",
    "from transformers import pipeline\n",
    "import torch\n",
    "\n",
    "generate_text = pipeline(model=\"databricks/dolly-v2-3b\", torch_dtype=torch.bfloat16,\n",
    "                         trust_remote_code=True, device_map=\"auto\", return_full_text=True)\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "id": "NBlbUrB27QQs"
   },
   "source": [
    "### Install Redis Stack\n",
    "\n",
    "Redis will be used as Vector Similarity Search engine for LangChain. Instead of using in-notebook Redis Stack you can provision your own free instance of Redis in the cloud. Get your own Free Redis Cloud instance at https://redis.io/try-free/ "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "aKMKXPY2j8Gt",
    "outputId": "f6fc107f-68c3-4413-ff4a-beb37d1c3076"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "deb [signed-by=/usr/share/keyrings/redis-archive-keyring.gpg] https://packages.redis.io/deb focal main\n",
      "Starting redis-stack-server, database path /var/lib/redis-stack\n"
     ]
    }
   ],
   "source": [
    "%%sh\n",
    "# curl -fsSL https://packages.redis.io/gpg | sudo gpg --dearmor -o /usr/share/keyrings/redis-archive-keyring.gpg   # COMMENTED OUT - Using Redis Cloud by default\n",
    "echo \"deb [signed-by=/usr/share/keyrings/redis-archive-keyring.gpg] https://packages.redis.io/deb $(lsb_release -cs) main\" | sudo tee /etc/apt/sources.list.d/redis.list \n",
    "sudo apt-get update  > /dev/null 2>&1\n",
    "# # sudo apt-get install redis-stack-server  > /dev/null 2>&1  # COMMENTED OUT - Using Redis Cloud by default  # COMMENTED OUT - Using Redis Cloud by default\n",
    "# redis-stack-server --daemonize yes   # COMMENTED OUT - Using Redis Cloud by default\n",
    "print(\"✅ Using Redis Cloud - no local installation needed!\")\n",
    ""
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "id": "X7UsU1Ts7TUL"
   },
   "source": [
    "### Connect to Redis\n",
    "\n",
    "By default this notebook would connect to the local instance of Redis Stack. If you have your own Redis Cloud instance - replace REDIS_PASSWORD, REDIS_HOST and REDIS_PORT values with your own."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "dyPfCO3pkB7M"
   },
   "outputs": [],
   "source": [
    "import redis\n",
    "import os\n",
    "\n",
    "# Redis Cloud Configuration (Default)\n",
    "# Using Redis Cloud for persistent data storage\n",
    "\n",
    "REDIS_HOST = os.getenv(\"REDIS_HOST\", \"redis-15306.c329.us-east4-1.gce.redns.redis-cloud.com\")\n",
    "REDIS_PORT = os.getenv(\"REDIS_PORT\", \"15306\")\n",
    "REDIS_PASSWORD = os.getenv(\"REDIS_PASSWORD\", \"ftswOcgMB8dLCnjbKMMDBg3DNnpi1KO5\")\n",
    "REDIS_USERNAME = os.getenv(\"REDIS_USERNAME\", \"default\")\n",
    "#Replace values above with your own if using Redis Cloud instance\n",
    "# Example: REDIS_HOST=\"redis-15306.c329.us-east4-1.gce.redns.redis-cloud.com\"\n",
    "#REDIS_PORT=18374\n",
    "#REDIS_PASSWORD=\"1TNxTEdYRDgIDKM2gDfasupCADXXXX\"\n",
    "\n",
    "#shortcut for redis-cli $REDIS_CONN command\n",
    "# If SSL is enabled on the endpoint add --tls\n",
    "if REDIS_PASSWORD!=\"\":\n",
    "  os.environ[\"REDIS_CONN\"]=f\"-h {REDIS_HOST} -p {REDIS_PORT} -a {REDIS_PASSWORD} --no-auth-warning\"\n",
    "else:\n",
    "  os.environ[\"REDIS_CONN\"]=f\"-h {REDIS_HOST} -p {REDIS_PORT}\"\n",
    "\n",
    "# If SSL is enabled on the endpoint, use rediss:// as the URL prefix\n",
    "REDIS_URL = f\"redis://:{REDIS_PASSWORD}@{REDIS_HOST}:{REDIS_PORT}\"\n",
    "INDEX_NAME = f\"qna:idx\"\n",
    "\n",
    "# Test Redis connection\n",
    "!redis-cli $REDIS_CONN PING\n",
    "print(f\"🔗 Connecting to Redis Cloud: {REDIS_HOST}:{REDIS_PORT}\")\n",
    ""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "_h1e-L9yZfaY"
   },
   "outputs": [],
   "source": [
    "from langchain.vectorstores.redis import Redis\n",
    "from langchain.text_splitter import CharacterTextSplitter\n",
    "from langchain.chains import RetrievalQA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "9XL4P-k1d0Pi",
    "outputId": "11d329c0-9e0e-4f96-cee5-55cee90b6541"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--2023-06-12 18:23:04--  https://raw.githubusercontent.com/hwchase17/chat-your-data/master/state_of_the_union.txt\n",
      "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.108.133, 185.199.109.133, 185.199.110.133, ...\n",
      "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.108.133|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 39027 (38K) [text/plain]\n",
      "Saving to: ‘state_of_the_union.txt’\n",
      "\n",
      "state_of_the_union. 100%[===================>]  38.11K  --.-KB/s    in 0.003s  \n",
      "\n",
      "2023-06-12 18:23:04 (12.8 MB/s) - ‘state_of_the_union.txt’ saved [39027/39027]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "!wget https://raw.githubusercontent.com/hwchase17/chat-your-data/master/state_of_the_union.txt"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "id": "KXDbABtw-vAQ"
   },
   "source": [
    "### Load text and split it into manageable chunks\n",
    "\n",
    "Without this step any large body of text would exceed the limit of tokens you can feed to the LLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "4zfOlQeeZjsN"
   },
   "outputs": [],
   "source": [
    "from langchain.document_loaders import TextLoader\n",
    "loader = TextLoader(\"./state_of_the_union.txt\")\n",
    "documents = loader.load()\n",
    "text_splitter = CharacterTextSplitter(chunk_size=500, chunk_overlap=30)\n",
    "texts = text_splitter.split_documents(documents)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "id": "Dv1JVm31_OF9"
   },
   "source": [
    "### Initialize embeddings engine\n",
    "\n",
    "Here we are using https://huggingface.co/sentence-transformers/all-MiniLM-L6-v2 which is a compact embedding engine that can be run locally. For large number of embeddings/documents you might consider GPU - enabled runtime or calling the hosted Embeddings API."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 633,
     "referenced_widgets": [
      "9f12f7712ce54b509c5a919406f4fc0d",
      "f2e743ecb39d4fe2aeacb908a50af63b",
      "6d912ee2b16846419e4fc4f0908aa8dd",
      "3111e8e8b8af421881b79b0561de48ec",
      "0b61748f48b446898b6629fb80202404",
      "91f1b83370c24e69ba64ef3df9bd5e2c",
      "56270a747b82464fb5ffd422c3c6546c",
      "e3e21303949949629aa82f6be3419c21",
      "0d02966ad43c4c568730d945994927be",
      "f7ccac7b6eac49adaa0014bde9ee98ea",
      "6a7a7cd66c2845c0a42e0dccf26b8bb9",
      "d1fdf9e3f99841589ac277d30073ddb3",
      "b487d429dd5241a7a272e8a192446aaf",
      "3f214f6817444f81b87d2f8c47406375",
      "0202c3913d4f4a1caab45e296837b01e",
      "95a5628de05c4e50870d416609c2b3ce",
      "af338197474141799f4cafdbddd944d4",
      "51bf61eab90e4de087d3a4c3f2c0d0d1",
      "ee555283a2fc42549a9e9da907c9aa3d",
      "ec81d1e5bf2f442084e9dd56cbe5fcb2",
      "95458a5fa25042f5b36445cde40d28d7",
      "c9b0b265dfa8480aa0ee1f78df149d86",
      "420bfffb1fbb4ae394837a089ef0fde7",
      "94b928f23c64461aadab8497e9c505c6",
      "e34df2a7bee643f084196e1645af0b49",
      "9ba437571f434ea2bc2f4712063fba76",
      "c75c3fc5ce29426f9b68d4fdf8b4cb34",
      "863730f09bb74cfda752da376eab9f2e",
      "71d06b5e386949949bbea1ffddf29c2f",
      "ff6fe95cd73747b4b508e7949a633d4e",
      "dbbfc24b2b624ddc9ce4294f7bfe704a",
      "a511761659524dd3bc87f4e82d1286fd",
      "5f8663632c4349bfb66ddc0864ffd2f2",
      "538dc90df8e5495e883dc10b73229cd4",
      "58a8da39a27f46289c38ca209aa01a41",
      "867ce3f6e4b54d01bf2de92c96c85bfd",
      "b69d5073edcc4bdfbe7d0316115108e1",
      "afbb73cddad641beb5514e3b956c8237",
      "c73e3f283100499d8a38f36193e6df48",
      "e9256867d61e4972a97183f556348dae",
      "2fb9f7b5eeb74bb89b0097a736cfeb4d",
      "8cb0e1b2e9f84827a96eb31341ac44db",
      "24b0b1a7d3a2487db81e426490f72963",
      "bc235af661e9474c9fdaa4847b5e6485",
      "f3c08839d6164e6c89fdb15b90869e07",
      "40100024f6f840d49c93f46bc896aaec",
      "bd21b00a00a5498690276fb58cfa8db1",
      "45da77e2daef46edbd6a1b91776908e4",
      "e7b659e80288420795cd03febf158339",
      "abd601b801b7481e919785d31fb2e8cd",
      "f9d63bf04a694a4a87af6a7f4bc5d3b8",
      "6ca9c3b4bec947ae98e5325e6f9f5580",
      "c265afed781a4b9daac1c73198461528",
      "f0481af29a154789ae18b755426a781f",
      "6cfaf9faea184a42a186b91c61e41389",
      "7103796dd6424c5f82bc91d1ae02122d",
      "df83af7d544b4f0aa8bac831e3b2acc1",
      "6be3982451d64331979ff18b29a45a1e",
      "b35bd7fd1f3542d6b08cf14b1bbf5ba2",
      "35d7278183734cb48d941d539e7223b7",
      "b190ef319f4b45b9b52ebdfd8a9eac7c",
      "59dc69fa755c4cab8b48e1ca34ed6cd1",
      "ae42b619d783424288a795f4bd71faf2",
      "c534f054e8764bce90f49b4f7b9a8f7a",
      "32edfb38a41b472192ca6b90fec5a455",
      "d76d27381b12462a9859777379236e5b",
      "be4bc558206647b38f63e9ee6191a5f3",
      "e7a8afd53192403e8dfcba80952a1538",
      "084fbd5e22c74a52a565cce1cd97005a",
      "5dcd3217ac9545248ca9967829873aec",
      "7e4e1b83e2914bd5a2b2029222fd016a",
      "88c6a4e87fc5482081d23e910d2af7e7",
      "a8418465b5af49e2940102d6f50d059c",
      "0bd0cad79c3d4cc2b9fcd45645cb0e83",
      "41023bb0c369406bb58d951e811c78e1",
      "4c2261159f4646ef99855fadd1f27ce5",
      "b4db3114be4048cd84cca7df54f78fee",
      "4daa047483b64fcf86f9eb1f398b86e5",
      "852daecd87af4aab847cc2a6dec97db3",
      "68f98a3a1f9e4e72952e42aa7be42816",
      "af011cb610e24580882634f07ecabb67",
      "a3e4d978d8f844c19a1ac36de46ca400",
      "12d1ff4ed1394950b58c827b36f0e930",
      "d51ee50851664fa29bd8d16c0336899c",
      "3e03fcd5f4c34ab3a8d3b635046eb6fa",
      "f34e252a3dcb4c71b62ab22c220e7b9f",
      "dd8f9d8f30f948e0b8398b4534dff07e",
      "2c7a5d8096c6472ca2301b149683ef3f",
      "66203933165f4ff09593fc159b88a717",
      "7aa35bd49d2540f6bed40cb6a3a6fd2e",
      "34a97f88f90745f794e155ded68484fa",
      "b2dd87dba980498c87c7f8ba47400a98",
      "669a507f0443429cb4a9c4538a717115",
      "b1ec743567cf4f889178c213336a2539",
      "b943fc239b844b2896c7efd1ce003f76",
      "3fd0465f4f8f4c4dbbe1373f3415c886",
      "9d45613b6aa944bda0df24103702a837",
      "2b7b5394c9b3450c8858990818b36e9e",
      "21e122635a734576900a611a954967fb",
      "c9e21049cda24fd9ac06a739ece4d0e0",
      "ce646396fa52423581ec65a8cfdeeb6a",
      "b2477c73dbf8466fb6a9956893d20c2f",
      "1220c9f678f8413b9a1aa19f5da1d81a",
      "c703eb9cd2384acdbbde77f9d2afb9c3",
      "ddfb571a433a4bf7b00f22da3add65a0",
      "5a4aff5ace2e4a6c9430c5991f51f425",
      "0a2a12792ae94c3fb6ed21e6be55e886",
      "0ace757cb7d643d28f3cbd2bb6cb7279",
      "9c9356508c46444a8d353c81b784b3da",
      "8f9c81db589146d4a827a2deee272dde",
      "5f803c4a979f4ebeb2728b0119e39945",
      "d533010647484a84bf9a5e77d01b5242",
      "55377af67a4c4c33a9e332bcc747176a",
      "7eee44d741c543c48ab9215cc8e8139a",
      "7e01929ef94543a6af2e8528853ea2a8",
      "1013af673e88468b8d05e8dd882baea5",
      "c77006af03ca412c8f7a068dadab3953",
      "0d0b8c0de7c348b5b2522991b5a98a09",
      "8b5cd2dc742e48ee9102d25583ae2b4b",
      "2a371671cfee40ae889608559c26376f",
      "35f5c2bb2ec54c858901d4d83de84619",
      "4d1eafc2476e487fbb0c7e01f41db44c",
      "d1de63be578647af9e0bc6086f27c4a1",
      "8f329a81aa9f47b39c9b1b45d4450f2e",
      "90718f750ab6453c82560902beea932f",
      "d1cca68d301e401dbf5e73686f6f0aab",
      "ce31a55d054f46119f234ea203b9ff05",
      "652a7a4e551b4276b814733d5fae58f4",
      "f61555f6a33b4ef39f960286f68a9d82",
      "7730c75e54874cb099e298c1a25f4229",
      "cc9428ee1cf049eaa06d1bffb0277085",
      "73415ce4a4194a2e94b5e17f816d1022",
      "7ef8c3a14be047d4b177cb7e60b9f022",
      "e9cd186f279b4894be0f3b33da48d518",
      "552cdaf2704f4f1088c7029d54f89bf0",
      "fc578ae55b4647418adc73a2c084e948",
      "a6d515198c2e4296a149459240676fb7",
      "7a26158c3c8842b1a0b3783992360bb7",
      "1e4a1049bb05438585c96354f36319a8",
      "5c9d71772582464e97ba669d174ebfa1",
      "f9e01ac6133d43f98af43c9d0c383ef5",
      "db7380d6357345acaa43aa722b08a5dc",
      "5888295914aa44d5ba4e297a57a557ef",
      "3a0d56224b944c0a8d733f264b958c83",
      "cf562fe748e24330afebff7ce766dc2d",
      "544953d28add4e02a1554389a872cf3a",
      "0dae1b4235b64d19af84e53f0ae7c362",
      "110a2969da8d44f5b3637069bda250d5",
      "553a45505be24f538b64a15445451725",
      "9a9a5cf1ec69442c90bd874dfcf9e6b7",
      "c4e770eb66434b078ecc1af4f684a3d7",
      "c0a742838a1b43808083fb6c394371e0",
      "923e4a37ecbd4798902e90dc766300d8",
      "0b2dd1092f8742eb929631d5b0595317"
     ]
    },
    "id": "gaJrhuKa_Mwt",
    "outputId": "3c609376-92be-49d0-ad5d-b1d5d61ebdeb"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9f12f7712ce54b509c5a919406f4fc0d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)e9125/.gitattributes:   0%|          | 0.00/1.18k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d1fdf9e3f99841589ac277d30073ddb3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)_Pooling/config.json:   0%|          | 0.00/190 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "420bfffb1fbb4ae394837a089ef0fde7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)7e55de9125/README.md:   0%|          | 0.00/10.6k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "538dc90df8e5495e883dc10b73229cd4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)55de9125/config.json:   0%|          | 0.00/612 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f3c08839d6164e6c89fdb15b90869e07",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)ce_transformers.json:   0%|          | 0.00/116 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7103796dd6424c5f82bc91d1ae02122d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)125/data_config.json:   0%|          | 0.00/39.3k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "be4bc558206647b38f63e9ee6191a5f3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading pytorch_model.bin:   0%|          | 0.00/90.9M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4daa047483b64fcf86f9eb1f398b86e5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)nce_bert_config.json:   0%|          | 0.00/53.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "66203933165f4ff09593fc159b88a717",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)cial_tokens_map.json:   0%|          | 0.00/112 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c9e21049cda24fd9ac06a739ece4d0e0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)e9125/tokenizer.json:   0%|          | 0.00/466k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5f803c4a979f4ebeb2728b0119e39945",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)okenizer_config.json:   0%|          | 0.00/350 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4d1eafc2476e487fbb0c7e01f41db44c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)9125/train_script.py:   0%|          | 0.00/13.2k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7ef8c3a14be047d4b177cb7e60b9f022",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)7e55de9125/vocab.txt:   0%|          | 0.00/232k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3a0d56224b944c0a8d733f264b958c83",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)5de9125/modules.json:   0%|          | 0.00/349 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def get_embeddings():\n",
    "  from langchain.embeddings import HuggingFaceEmbeddings\n",
    "  embeddings = HuggingFaceEmbeddings(model_name=\"all-MiniLM-L6-v2\")\n",
    "  return embeddings\n",
    "\n",
    "embeddings = get_embeddings()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "id": "6zgvqB6wCJWK"
   },
   "source": [
    "### Initialize LLM\n",
    "\n",
    "In this notebook we are using Databricks Dolly-v2-3b LLM that we loaded earlier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "iQRyyWU0CNbJ"
   },
   "outputs": [],
   "source": [
    "#llm = OpenAI()\n",
    "llm = HuggingFacePipeline(pipeline=generate_text)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "id": "Qrj-jeGmBRTL"
   },
   "source": [
    "### Create vector store from the documents using Redis as Vector Database"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "id": "yY69FViAjNv1"
   },
   "outputs": [],
   "source": [
    "def get_vectorstore() -> Redis:\n",
    "    \"\"\"Create the Redis vectorstore.\"\"\"\n",
    "\n",
    "    try:\n",
    "        vectorstore = Redis.from_existing_index(\n",
    "            embedding=embeddings,\n",
    "            index_name=INDEX_NAME,\n",
    "            redis_url=REDIS_URL\n",
    "        )\n",
    "        return vectorstore\n",
    "    except:\n",
    "        pass\n",
    "\n",
    "    # Load Redis with documents\n",
    "    vectorstore = Redis.from_documents(\n",
    "        documents=texts,\n",
    "        embedding=embeddings,\n",
    "        index_name=INDEX_NAME,\n",
    "        redis_url=REDIS_URL\n",
    "    )\n",
    "    return vectorstore\n",
    "\n",
    "\n",
    "redis = get_vectorstore()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "id": "XdzQa112Bf2b"
   },
   "source": [
    "## Specify the prompt template\n",
    "\n",
    "PromptTemplate defines the exact text of the response that would be fed to the LLM. This step is optional, but the defaults usually work well for OpenAI and might fall short for other models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "id": "yKKn2KKp3TQ4"
   },
   "outputs": [],
   "source": [
    "def get_prompt():\n",
    "    \"\"\"Create the QA chain.\"\"\"\n",
    "    from langchain.prompts import PromptTemplate\n",
    "    from langchain.chains import RetrievalQA\n",
    "\n",
    "    # Define our prompt\n",
    "    prompt_template = \"\"\"Use the following pieces of context to answer the question at the end. If you don't know the answer, say that you don't know, don't try to make up an answer.\n",
    "\n",
    "    This should be in the following format:\n",
    "\n",
    "    Question: [question here]\n",
    "    Answer: [answer here]\n",
    "\n",
    "    Begin!\n",
    "\n",
    "    Context:\n",
    "    ---------\n",
    "    {context}\n",
    "    ---------\n",
    "    Question: {question}\n",
    "    Answer:\"\"\"\n",
    "\n",
    "    prompt = PromptTemplate(\n",
    "        template=prompt_template,\n",
    "        input_variables=[\"context\", \"question\"]\n",
    "    )\n",
    "    return prompt"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "id": "xgEXBujxG1dO"
   },
   "source": [
    "### Putting it all together\n",
    "\n",
    "This is where the LangChain brings all the components together in a form of a simple QnA chain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "id": "RKNSP0zqZq98"
   },
   "outputs": [],
   "source": [
    "qa = RetrievalQA.from_chain_type(\n",
    "    llm=llm, \n",
    "    chain_type=\"stuff\", \n",
    "    retriever=redis.as_retriever(),\n",
    "    #return_source_documents=True,\n",
    "    chain_type_kwargs={\"prompt\": get_prompt()}\n",
    "    )"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "id": "-ox7LffJ8VNe"
   },
   "source": [
    "### Debugging Redis\n",
    "\n",
    "The code block below is example of how you can interact with the Redis Database"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "id": "rGH8mdG2na0w"
   },
   "outputs": [],
   "source": [
    "#!redis-cli $REDIS_CONN keys \"*\"\n",
    "#!redis-cli $REDIS_CONN HGETALL \"doc:qna:idx:063955c855a7436fbf9829821332ed2a\"\n",
    "\n",
    "###-- FLUSHDB will wipe out the entire database!!! Use with caution --###\n",
    "#!redis-cli $REDIS_CONN flushdb\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "id": "SURTtVbYBFGc"
   },
   "source": [
    "## Finally - let's ask questions! \n",
    "\n",
    "Examples:\n",
    "- What did the president say about Ketanji Brown Jackson\n",
    "- Did he mention Stephen Breyer?\n",
    "- What was his stance on Ukraine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 89
    },
    "id": "0JkswfOHZu9h",
    "outputId": "c717c85d-28fa-45ba-d976-f075409ed6b9"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.google.colaboratory.intrinsic+json": {
       "type": "string"
      },
      "text/plain": [
       "'\\nPresident Obama: \"One of the most serious constitutional responsibilities a President has is nominating someone to serve on the United States Supreme Court. And I did that 4 days ago, when I nominated Circuit Court of Appeals Judge Ketanji Brown Jackson. One of our nation’s top legal minds, who will continue Justice Breyer’s legacy of excellence.\"'"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "\n",
    "query = \"What did the president say about Ketanji Brown Jackson\"\n",
    "qa.run(query)"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "include_colab_link": true,
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python (.venv)",
   "language": "python",
   "name": ".venv"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}